% THIS TEMPLATE IS A WORK IN PROGRESS
% Adapted from an original template by faculty at Reykjavik University, Iceland

\documentclass{scrartcl}
\input{File_Setup.tex}
\hbadness=10000

 


\begin{document}
%Title of the report, name of coworkers and dates (of experiment and of report).
\begin{titlepage}
	\centering
	\includegraphics[width=0.6\textwidth]{nyush-logo.jpeg}\par
	\vspace{2cm}
	%%%% COMMENT OUT irrelevant lines among the 3 below
  {\scshape\LARGE Computer Science, Data Science \& \par}  %if you're a CS major
  {\scshape\LARGE Computer Systems Engineering \par}
	\vspace{1cm}
	{\scshape\Large Capstone Report - Fall 2024\par}
	%{\large \today\par}
	\vfill
	
	%%%% PROJECT TITLE
	{\huge\bfseries Benchmarking ZK Virtual Machines for Privacy-Preserving Machine Learning Applications\par}
	\vfill
	
	%%%% AUTHOR(S)
	{\Large\itshape Lawrence Lim\\ Siddhartha Tuladhar\\ Brandon Gao\\}\par
	\vspace{1.5cm}

	\vfill
	supervised by\par
	%%%% SUPERVISOR(S)
  Promethee Spathis

	\vfill
% Bottom of the page
\end{titlepage}

\newpage

\begin{preface}
 As a team comprising a Computer Systems Engineering major, a Computer Science major, and a Data Science major, we bring diverse perspectives and expertise to address the complex challenges at the intersection of privacy, security, and scalability in technology. This project was inspired by the increasing importance of privacy-preserving computation, particularly in sensitive fields like finance, where secure data handling is paramount. Our collective academic backgrounds have allowed us to explore innovative approaches to these challenges, drawing from distributed systems, cryptography, and data analytics.

Our target audience includes researchers, developers, and industry professionals who are advancing privacy technologies, blockchain systems, and secure data frameworks. By benchmarking zero-knowledge virtual machines (zkVMs) in the context of financial data, this project seeks to provide valuable insights into their capabilities and limitations, contributing to the ongoing development of secure and privacy-centric computational tools.       
\end{preface}

\vspace{1cm}

\begin{acknowledgements}
We sincerely thank our advisor, Professor Promethee Spathis, for their guidance and support throughout this project. We are also grateful to the Professor Benedikt Bunz for providing the initial ideation for this project. Lastly, we are grateful to our families and friends for their encouragement and support.\end{acknowledgements}

\newpage

\begin{abstract}
This work addresses the challenge of securely processing sensitive data in privacy-critical applications like finance. Zero-knowledge virtual machines (zkVMs) offer a promising solution, but face issues with complexity and proof generation time. We benchmark three zkVMs—SP1, Jolt, and RISC-0—by training a ridge regression model on financial data, evaluating their performance and identifying key bottlenecks. Our findings highlight zkVMs’ potential for privacy-preserving computation and provide insights for improving their practical adoption.
\end{abstract}
\vspace{1cm}

\begin{keywords}
\centering
         \textbf{Capstone; Computer science; Machine Learning; Zero-Knowledge Proofs, Zero-Knowledge Virtual Machines, Jolt, SP1, Risc0, NYU Shanghai}
\end{keywords}

\newpage



\doublespacing
\tableofcontents
\singlespacing

\newpage

\doublespacing

\section{Introduction}

\subsection{Context}

Currently, there exists a large amount of sensitive customer data that is extremely valuable but not being monetized to its fullest extent due to a combination of compliance and ethical concerns. An essential category of this data is personal financial data. Due to the sensitive nature of this data and strict compliance laws, current Fintech platforms require users’ explicit permission to access sensitive information like credit history, transaction history, and income. However, \textbf{Zero Knowledge Proof (ZKP)} can be utilized to develop services and algorithms that utilize the sensitive data without explicitly revealing it. A ZKP is a cryptographic method of proving a statement is true without revealing any other information besides the fact that the statement is true. ZKPs have three fundamental characteristics:

\begin{itemize}
    \item \textbf{Completeness:} If a statement is true, an honest prover can prove to an honest verifier that they have knowledge of the correct input.
    \item \textbf{Soundness:} If a statement is false, a dishonest prover is unable to convince an honest verifier that they have knowledge of the correct input.
    \item \textbf{Zero-knowledge:} No other information about the input is revealed to the verifier from the prover besides the fact that the statement is true.
\end{itemize}

Currently, the most-friendly way of generating a ZKP is through the use of zero-knowledge virtual machines (zkVMs). A zkVM, is simply a VM implemented as a circuit for a ZKP system. So, instead of proving the execution of a program, as one would normally do in ZKP systems, you prove the execution of the bytecode of a given Instruction Set Architecture (ISA). However, the zkVM landscape is in early development, and proof generation is bottlenecked by the complexity of the program and hardware limitations. In this paper, we provide a quantatitve and qualitative analysis on the current state of zkVMs on a real-world use case by generating a proof of a machine learning (ML) algorithm on dummy financial data.

\subsection{Objective}

The objective of this paper is to provide researchers, developers, and industry professionals a comprehensive review on utilizing zkVMs in a real-world use case. In order to achieve this, we benchmark SP1\cite{Roy2024}, Risc0\cite{bruestle2023risc}, and Jolt\cite{arun2024jolt}, since they all compile to RISC-V, which is an extremely popular compile target for most programming languages (Rust, C++, LLVM). We provide a quantative analysis on proof generation time and proof verification time. Additionally, we provide a qualitative analysis on the developer experience on developing ML algorithms for these zkVMs. The ML algorithm that we use is the ridge regression model. 

\section{Related Work}

%%%% DO NOT write a related work section that is just a laundry list of other papers, with a sentence about each one that was lifted from its abstract, and without any critical analysis nor deep comparison to other work.

\subsection{Virtual Machine-based ZK Systems}

 Our research explores the most efficient methods for generating ZKPs for machine learning algorithms by experimenting with different zkVMs. The current papers on zkVMs focus on their architecture and the techniques that are used by the zkVM to achieve optimized proof and verification times. For example, Arun et al.\cite{arun2024jolt} present Jolt, a zero-knowledge Succinct Non-Interactive Argument of Knowledge (zkSNARK) system optimized for VMs, which uses a new lookup technique to reduce proof size and verification time. Their paper focuses on this optimization that makes Jolt effective in environments where complex computations occur, and proof generation must remain efficient. Similarly, Bruestle and Gafni\cite{bruestle2023risc} introduce RISC-0, a system designed to produce scalable, transparent proofs for computations based on the RISC-V instruction set. RISC-0 focuses on architecture-specific applications, such as the RISC-V ecosystem, which contrasts with Jolt’s broader application to virtual machines. Currently, there is no literature on the direct comparison between these zkVMs, which is the objective of our paper. Additionally, there is no research or comparisons on SP1. 

\subsection{Machine Learning in ZKPs}

Machine learning is a highly relevant use case for ZKPs. There are multiple articles on the various use cases of ML in ZKPs. Wang et al.\cite{wang2024efficient} developed an efficient ZKP-based pipeline for classical machine learning inference, focusing on privacy-preserving inference that ensures model accuracy without exposing sensitive model information. This system optimizes zero-knowledge inference, making it highly applicable to real-world ML applications where data privacy is paramount. Our paper uses the same inference technique within the zkVM as a real world application. Similarly, Sathe et al.\cite{sathe2024survey} provide a comprehensive survey of ZKP applications in machine learning, covering a range of cryptographic techniques and their applicability to neural networks, decision trees, and other ML models. Their survey offers a broader comparison of ZKP techniques in the ML space, providing essential context for more specific systems like that of Wang et al. However, their paper does not provide details on the performance or real-world use case. Lastly, Ganescu and Passerat-Palmbach\cite{ganescu2024trust} extend the application of ZKPs to generative AI models, by proposing a system that enhances trust in AI by allowing for the verification of generative models without exposing underlying data, which is critical in AI-driven environments where sensitive inputs and outputs must be safeguarded while still proving the model’s validity. This paper is similar to Wang et al., where it provides a real-world use case, but it utilizes generating ZKPs through tools that are inaccessible to developers and require high development overhead. By using zkVMs, most of the ZKP logic is abstracted away, which provides developers easier access to converting their programs into ZKPs.

\section{Solution}

The solution section covers all of your contributions (architecture, algorithms, formulas, findings).
It explains in detail each contribution, if possible with figures/schematics.

Don't forget that a figure goes a long way towards helping your reader understand your work. For instance, Figure~\ref{fig:ascent} outlines the layers involved in a distributed certification service, and how they articulate together. Nevertheless, a figure must always come with at least one paragraph of explanation. The rule is that anyone should be able to understand your solution from reading the text in this section, even if they skip the figures.

To benchmark the zkVMs,


\textbf{Polynomial Ridge Regression} is a nonlinear extension of ridge regression that leverages polynomial feature transformation to capture complex relationships between variables. Polynomial ridge regression enhances ridge regression by generating polynomial terms to capture nonlinear patterns. In this model, polynomial features are created, such as:

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{n+1} x_1^2 + \beta_{n+2} x_2^2 + \beta_{k} (x_i \times x_j) + \epsilon
\]

Ridge regularization is then applied to these expanded features, controlling overfitting while accommodating non-linear relationships. The objective function becomes:

\[
Objective = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{m} \beta_j^2
\]

where \( m \) represents the number of polynomial features. This regularization prevents overfitting by penalizing large coefficients. This combination enables the model to capture nonlinear relationships while controlling for overfitting through regularization.

\begin{table}[h!]
\centering
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Field} & \textbf{Description} \\
\hline
\textbf{CustomerID} & Unique identifier for a customer, used to track and differentiate individuals in the dataset. \\
\hline
\textbf{Frequency} & Number of transactions a customer made during the first three quarters. \\
\hline
\textbf{Monetary} & Total monetary value of a customer's transactions during the first three quarters. \\
\hline
\textbf{Recency} & Number of days since the customer's last transaction during the first three quarters. \\
\hline
\textbf{Price} & Price per unit for each transaction item. \\
\hline
\textbf{DiscountApplied} & Percentage of discount applied to the transaction. \\
\hline
\textbf{spend\_90\_flag} & Binary flag indicating whether the customer made any transaction in the past 90 days. \\
\hline
\textbf{actual\_spend\_90\_days} & Total amount spent by the customer in the last 90 days(quarter). \\
\hline
\end{tabular}
\caption{Description of fields in the customer transaction dataset.}
\label{tab:dataset_description}
\end{table}

\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.35]{zkVM-design.png}
	\end{center}
	\caption{Architecture of our benchmark procedure}
	\label{fig:ascent}
\end{figure}

\section{Results}

The results section details your metrics and experiments for the assessment of your solution. It then provides experimental validation for your approach with visual aids such as data tables and graphs. In particular, it allows you to compare your idea with other approaches you've tested, for example solutions you've mentioned in your related work section.

\subsection{Experimentation Protocol}

It is of the utmost importance to describe how you came up with the measurements and results that support your evaluation.
%
% \subsection{Data tables}
%
% Every data table should be numbered, have a brief description as its title, and specify the units used. 
%
% As an example, Table compares the average latencies of native application calls to networked services. The experiments were conducted on an Apple MacBook Air 2010 with a CPU speed of 1.4GHz and a bus speed of 800MHz. Each data point is a mean over 20 instances of each call, after discarding both the lowest and the highest measurement.
%
% \subsection{Graphs}
%
% Graphs are often the most important information in your report; you should design and plot them with great care. A graph contains a lot of information in a short space. Graphs should be numbered and have a title. Their axes should be labelled, with the quantities and units specified. Make sure that individual data points (your measurements) stand out clearly. And of course, always associate your graph with text that explains your results, and outlines the conclusions you draw from these results.
%
\subsection{ARM Benchmark}

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.3]{ARM-Proving-Time.png}
	\end{center}
	\caption{ARM Proving Time}
	\label{graph:arm-proving-time}
\end{figure}

For example, Figure~\ref{graph:arm-proving-time} compares the efficiency of three different service architectures in eliminating adversarial behaviors. Every data point gives the probability that $k$ faulty/malicious nodes managed to participate in a computation that involves 32 nodes. In the absence of at least one reliable node ($k = 32$), the failure will go undetected ; but the results show that this case is extremely unlikely, regardless of the architecture. The most significant result pertains to $k = 16$: the reliable nodes detect the failure, but cannot reach a majority to recover. The graph shows that the \texttt{CORPS 5\%} architecture is much more resilient than the \texttt{DHT 30\%} architecture, by a magnitude of $10^{11}$.

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.3]{ARM-Verification-Time.png}
	\end{center}
	\caption{ARM Verification Time}
	\label{graph:arm-verification-time}
\end{figure}

\subsection{x86 Benchmark}

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.3]{x86-Proving-Time.png}
	\end{center}
	\caption{x86 Proving Time}
	\label{graph:x86-proving-time}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.3]{x86-Verification-Time.png}
	\end{center}
	\caption{x86 Verification Time}
	\label{graph:x86-verification-time}
\end{figure}


\section{Discussion}
The discussion section focuses on the main challenges/issues you had to overcome during the project. Outline what your approach does better than the ones you mentioned in your related work, and explain why. Do the same with issues where other solutions  outperform your own. Are there limitations to your approach? If so, what would you recommend towards removing/mitigating them? Given the experience you've gathered working on this project, are there other approaches that you feel are worth exploring?

\section{Personal Contributions}

\subsection{Siddhartha Tuladhar}

\subsection{Lawrence Lim}

\subsection{Brandon Gao}

\section{Conclusion}

Give a clear, short, and informative summary of all your important results. Answer the initial question(s) or respond to what you wanted to do, as stated in your introduction. It can be a short table or a list, and possibly one or two short comments or explanations. 

Target a reader who may not have time to read the whole report yet, but needs the results or the conclusions immediately. This is a typical situation in real life. Some readers will read your introduction and skip to your conclusion first, and read the whole report only later (if at all).

You may also draw perspectives. What's missing? In what directions could your work be extended?

\newpage
\singlespacing
\bibliographystyle{IEEEtran}
\bibliography{references}


%------ To create Appendix with additional stuff -------%
%\newpage
%\appendix
%\section{Appendix}
%Put data files, CAD drawings, additional sketches, etc.

\end{document}
 
