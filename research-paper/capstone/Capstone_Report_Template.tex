% THIS TEMPLATE IS A WORK IN PROGRESS
% Adapted from an original template by faculty at Reykjavik University, Iceland

\documentclass{scrartcl}
\input{File_Setup.tex}
\hbadness=10000

\begin{document}
%Title of the report, name of coworkers and dates (of experiment and of report).
\begin{titlepage}
	\centering
	\includegraphics[width=0.6\textwidth]{nyush-logo.jpeg}\par
	\vspace{2cm}
	%%%% COMMENT OUT irrelevant lines among the 3 below
  {\scshape\LARGE Computer Science, Data Science \& \par}  %if you're a CS major
  {\scshape\LARGE Computer Systems Engineering \par}
	\vspace{1cm}
	{\scshape\Large Capstone Report - Fall 2024\par}
	%{\large \today\par}
	\vfill
	
	%%%% PROJECT TITLE
	{\huge\bfseries Benchmarking ZK Virtual Machines for Privacy-Preserving Machine Learning Applications\par}
	\vfill
	
	%%%% AUTHOR(S)
	{\Large\itshape Lawrence Lim\\ Siddhartha Tuladhar\\ Brandon Gao\\}\par
	\vspace{1.5cm}

	\vfill
	supervised by\par
	%%%% SUPERVISOR(S)
  Promethee Spathis

	\vfill
% Bottom of the page
\end{titlepage}

\newpage

\begin{preface}
 As a team comprising a Computer Systems Engineering major, a Computer Science major, and a Data Science major, we bring diverse perspectives and expertise to address the complex challenges at the intersection of privacy, security, and scalability in technology. This project was inspired by the increasing importance of privacy-preserving computation, particularly in sensitive fields like finance, where secure data handling is paramount. Our collective academic backgrounds have allowed us to explore innovative approaches to these challenges, drawing from distributed systems, cryptography, and data analytics.

Our target audience includes researchers, developers, and industry professionals who are advancing privacy technologies, blockchain systems, and secure data frameworks. By benchmarking zero-knowledge virtual machines (zkVMs) in the context of financial data, this project seeks to provide valuable insights into their capabilities and limitations, contributing to the ongoing development of secure and privacy-centric computational tools.       
\end{preface}

\vspace{1cm}

\begin{acknowledgements}
We sincerely thank our advisor, Professor Promethee Spathis, for their guidance and support throughout this project. We are also grateful to the Professor Benedikt Bunz for providing the initial ideation for this project. Lastly, we are grateful to our families and friends for their encouragement and support.\end{acknowledgements}

\newpage

\begin{abstract}
This work addresses the challenge of securely processing sensitive data in privacy-critical applications like finance. Zero-knowledge virtual machines (zkVMs) offer a promising solution, but face issues with complexity and proof generation time. We benchmark three zkVMs—SP1, Jolt, and RISC-0—by training a ridge regression model on financial data, evaluating their performance and identifying key bottlenecks. Our findings highlight zkVMs’ potential for privacy-preserving computation and provide insights for improving their practical adoption.
\end{abstract}
\vspace{1cm}

\begin{keywords}
\centering
         \textbf{Capstone; Computer science; Machine Learning; Zero-Knowledge Proofs, Zero-Knowledge Virtual Machines, Jolt, SP1, Risc0, NYU Shanghai}
\end{keywords}

\newpage



\doublespacing
\tableofcontents
\singlespacing

\newpage

\doublespacing

\section{Introduction}

\subsection{Context}

Currently, there exists a large amount of sensitive customer data that is extremely valuable but not being monetized to its fullest extent due to a combination of compliance and ethical concerns. An essential category of this data is personal financial data. Due to the sensitive nature of this data and strict compliance laws, current Fintech platforms require users’ explicit permission to access sensitive information like credit history, transaction history, and income. However, \textbf{Zero Knowledge Proof (ZKP)} can be utilized to develop services and algorithms that utilize the sensitive data without explicitly revealing it. A ZKP is a cryptographic method of proving a statement is true without revealing any other information besides the fact that the statement is true. ZKPs have three fundamental characteristics:

\begin{itemize}
    \item \textbf{Completeness:} If a statement is true, an honest prover can prove to an honest verifier that they have knowledge of the correct input.
    \item \textbf{Soundness:} If a statement is false, a dishonest prover is unable to convince an honest verifier that they have knowledge of the correct input.
    \item \textbf{Zero-knowledge:} No other information about the input is revealed to the verifier from the prover besides the fact that the statement is true.
\end{itemize}

Currently, the most-friendly way of generating a ZKP is through the use of zero-knowledge virtual machines (zkVMs). A zkVM, is simply a VM implemented as a circuit for a ZKP system. So, instead of proving the execution of a program, as one would normally do in ZKP systems, you prove the execution of the bytecode of a given Instruction Set Architecture (ISA). However, the zkVM landscape is in early development, and proof generation is bottlenecked by the complexity of the program and hardware limitations. In this paper, we provide a quantatitve and qualitative analysis on the current state of zkVMs on a real-world use case by generating a proof of a machine learning (ML) algorithm on dummy financial data.

\subsection{Objective}

The objective of this paper is to provide researchers, developers, and industry professionals a comprehensive review on utilizing zkVMs in a real-world use case. In order to achieve this, we benchmark SP1\cite{Roy2024}, Risc0\cite{bruestle2023risc}, and Jolt\cite{arun2024jolt}, since they all compile to RISC-V, which is an extremely popular compile target for most programming languages (Rust, C++, LLVM). We provide a quantative analysis on proof generation time and proof verification time. Additionally, we provide a qualitative analysis on the developer experience on developing ML algorithms for these zkVMs. The ML algorithm that we use is the ridge regression model. 

\section{Related Work}

%%%% DO NOT write a related work section that is just a laundry list of other papers, with a sentence about each one that was lifted from its abstract, and without any critical analysis nor deep comparison to other work.

\subsection{Virtual Machine-based ZK Systems}

 Our research explores the most efficient methods for generating ZKPs for machine learning algorithms by experimenting with different zkVMs. The current papers on zkVMs focus on their architecture and the techniques that are used by the zkVM to achieve optimized proof and verification times. For example, Arun et al.\cite{arun2024jolt} present Jolt, a zero-knowledge Succinct Non-Interactive Argument of Knowledge (zkSNARK) system optimized for VMs, which uses a new lookup technique to reduce proof size and verification time. Their paper focuses on this optimization that makes Jolt effective in environments where complex computations occur, and proof generation must remain efficient. Similarly, Bruestle and Gafni\cite{bruestle2023risc} introduce RISC-0, a system designed to produce scalable, transparent proofs for computations based on the RISC-V instruction set. RISC-0 focuses on architecture-specific applications, such as the RISC-V ecosystem, which contrasts with Jolt’s broader application to virtual machines. Currently, there is no literature on the direct comparison between these zkVMs, which is the objective of our paper. Additionally, there is no research or comparisons on SP1. 

\subsection{Machine Learning in ZKPs}

Machine learning is a highly relevant use case for ZKPs. There are multiple articles on the various use cases of ML in ZKPs. Wang et al.\cite{wang2024efficient} developed an efficient ZKP-based pipeline for classical machine learning inference, focusing on privacy-preserving inference that ensures model accuracy without exposing sensitive model information. This system optimizes zero-knowledge inference, making it highly applicable to real-world ML applications where data privacy is paramount. Our paper uses the same inference technique within the zkVM as a real world application. Similarly, Sathe et al.\cite{sathe2024survey} provide a comprehensive survey of ZKP applications in machine learning, covering a range of cryptographic techniques and their applicability to neural networks, decision trees, and other ML models. Their survey offers a broader comparison of ZKP techniques in the ML space, providing essential context for more specific systems like that of Wang et al. However, their paper does not provide details on the performance or real-world use case. Lastly, Ganescu and Passerat-Palmbach\cite{ganescu2024trust} extend the application of ZKPs to generative AI models, by proposing a system that enhances trust in AI by allowing for the verification of generative models without exposing underlying data, which is critical in AI-driven environments where sensitive inputs and outputs must be safeguarded while still proving the model’s validity. This paper is similar to Wang et al., where it provides a real-world use case, but it utilizes generating ZKPs through tools that are inaccessible to developers and require high development overhead. By using zkVMs, most of the ZKP logic is abstracted away, which provides developers easier access to converting their programs into ZKPs.

\section{Solution}

The solution section covers all of your contributions (architecture, algorithms, formulas, findings).
It explains in detail each contribution, if possible with figures/schematics.

Don't forget that a figure goes a long way towards helping your reader understand your work. For instance, Figure~\ref{fig:ascent} outlines the layers involved in a distributed certification service, and how they articulate together. Nevertheless, a figure must always come with at least one paragraph of explanation. The rule is that anyone should be able to understand your solution from reading the text in this section, even if they skip the figures.

To benchmark the zkVMs, we trialed three machine learning algorithms.

\subsection{ML Setup}

The dummy transaction data we used was taken from a Kaggle dataset\cite{rehman2023retail}. %TODO: explain what we did with the data and how you formatted it.

\begin{table}[ht!]
\centering
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Field} & \textbf{Description} \\
\hline
\textbf{CustomerID} & Unique identifier for a customer, used to track and differentiate individuals in the dataset. \\
\hline
\textbf{Frequency} & Number of transactions a customer made during the first three quarters. \\
\hline
\textbf{Monetary} & Total monetary value of a customer's transactions during the first three quarters. \\
\hline
\textbf{Recency} & Number of days since the customer's last transaction during the first three quarters. \\
\hline
\textbf{Price} & Price per unit for each transaction item. \\
\hline
\textbf{DiscountApplied} & Percentage of discount applied to the transaction. \\
\hline
\textbf{spend\_90\_flag} & Binary flag indicating whether the customer made any transaction in the past 90 days. \\
\hline
\textbf{actual\_spend\_90\_days} & Total amount spent by the customer in the last 90 days(quarter). \\
\hline
\end{tabular}
\caption{Description of fields in the customer transaction dataset.}
\label{tab:dataset_description}
\end{table}

After preprocessing, the features that are left in the dataset are shown in Table \ref{tab:dataset_description}.

For the machine learning algorithm, we used the Ridge Regression Model due to the model's accuracy and the complexity constraints of the zkVMs. Ridge Regression is an extension of linear regression by adding a penalty term to the loss function to prevent overfitting.  Ridge Regression prevents overfitting by penalizing large coefficient values, thereby shrinking them towards zero. The Ridge Regression loss function is formulated as:

\[
\text{Loss} = \sum_{i=1}^{m} \left( y_i - \hat{y}_i \right)^2 + \lambda \sum_{j=1}^{n} w_j^2
\]

where \( m \) is the number of samples, \( n \) is the number of features, \( \hat{y}_i \) represents the predicted value for the \( i \)-th sample, \( \lambda \) is the regularization parameter controlling the strength of the penalty, and \( w_j \) are the model coefficients. The second term \( \lambda \sum_{j=1}^{n} w_j^2 \) is the regularization term, which discourages large coefficients and helps reduce the model's variance. The accuracy in 

\subsection{zkVM Implementation}



\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.35]{zkVM-design.png}
	\end{center}
	\caption{Architecture of our benchmark procedure}
	\label{fig:ascent}
\end{figure}

\section{Results}

The results section details your metrics and experiments for the assessment of your solution. It then provides experimental validation for your approach with visual aids such as data tables and graphs. In particular, it allows you to compare your idea with other approaches you've tested, for example solutions you've mentioned in your related work section.

% \subsection{Data tables}
%
% Every data table should be numbered, have a brief description as its title, and specify the units used. 
%
% As an example, Table compares the average latencies of native application calls to networked services. The experiments were conducted on an Apple MacBook Air 2010 with a CPU speed of 1.4GHz and a bus speed of 800MHz. Each data point is a mean over 20 instances of each call, after discarding both the lowest and the highest measurement.
%
% \subsection{Graphs}
%
% Graphs are often the most important information in your report; you should design and plot them with great care. A graph contains a lot of information in a short space. Graphs should be numbered and have a title. Their axes should be labelled, with the quantities and units specified. Make sure that individual data points (your measurements) stand out clearly. And of course, always associate your graph with text that explains your results, and outlines the conclusions you draw from these results.
%
\subsection{ARM Benchmark}

\begin{table}[h!]
	\centering
	\caption{Proving and Verifying Times for Different zkVMs on ARM}
	\begin{tabular}{|c|c|c|c|}
	\hline
	\textbf{Framework} & \textbf{10} & \textbf{100} & \textbf{1000} \\ \hline
	\multicolumn{4}{|c|}{\textbf{Proving}} \\ \hline
	SP1 & 7.46 & 18.94 & 132.13 \\ \hline
	RISCO & 4.92 & 18.66 & 167.85 \\ \hline
	Jolt & 13.57 & 19.61 & 75.10 \\ \hline
	\multicolumn{4}{|c|}{\textbf{Verifying}} \\ \hline
	SP1 & 167.40 & 170.68 & 818.74 \\ \hline
	RISCO & 22.73 & 25.41 & 124.53 \\ \hline
	Jolt & 259.08 & 280.71 & 345.82 \\ \hline
	\end{tabular}
	\label{tab:proving-verifying-times}
	\end{table}

\begin{figure}
	\begin{center}
		\includegraphics[width=\linewidth]{ARM-Proving-Time.png}
	\end{center}
	\caption{ARM Proving Time}
	\label{graph:arm-proving-time}
\end{figure}

For example, Figure~\ref{graph:arm-proving-time} compares the efficiency of three different service architectures in eliminating adversarial behaviors. Every data point gives the probability that $k$ faulty/malicious nodes managed to participate in a computation that involves 32 nodes. In the absence of at least one reliable node ($k = 32$), the failure will go undetected ; but the results show that this case is extremely unlikely, regardless of the architecture. The most significant result pertains to $k = 16$: the reliable nodes detect the failure, but cannot reach a majority to recover. The graph shows that the \texttt{CORPS 5\%} architecture is much more resilient than the \texttt{DHT 30\%} architecture, by a magnitude of $10^{11}$.

\begin{figure}
	\begin{center}
		\includegraphics[width=\linewidth]{ARM-Verification-Time.png}
	\end{center}
	\caption{ARM Verification Time}
	\label{graph:arm-verification-time}
\end{figure}

\subsection{x86 Benchmark}
\begin{table}[h!]
	\centering
	\caption{Proving and Verifying Times for Different zkVMs on X86}
	\begin{tabular}{|c|c|c|c|}
	\hline
	\textbf{Framework} & \textbf{10} & \textbf{100} & \textbf{1000} \\ \hline
	\multicolumn{4}{|c|}{\textbf{Proving}} \\ \hline
	SP1 & 11.93 & 30.83 & 117.45 \\ \hline
	RISCO & 9.62 & 36.37 & 309.14 \\ \hline
	Jolt & 13.62 & 19.31 & 69.36 \\ \hline
	\multicolumn{4}{|c|}{\textbf{Verifying}} \\ \hline
	SP1 & 230.40 & 219.54 & 372.52 \\ \hline
	RISCO & 19.86 & 21.34 & 101.21 \\ \hline
	Jolt & 489.86 & 281.02 & 555.56 \\ \hline
	\end{tabular}
	\label{tab:proving-verifying-times}
	\end{table}
	

\begin{figure}
	\begin{center}
		\includegraphics[width=\linewidth]{x86-Proving-Time.png}
	\end{center}
	\caption{x86 Proving Time}
	\label{graph:x86-proving-time}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=\linewidth]{x86-Verification-Time.png}
	\end{center}
	\caption{x86 Verification Time}
	\label{graph:x86-verification-time}
\end{figure}

\subsection{Performance Metrics}
\textbf{\(R^2\) (Coefficient of Determination):} 
The \(R^2\) value measures the proportion of variance in the target variable that is explained by the independent variables in the model. It ranges from 0 to 1, where higher values indicate a better fit of the model to the data.

\textbf{Mean Square Error (MSE):} 
The MSE represents the average squared difference between the predicted and actual values. It provides a measure of the model's prediction error, with lower values indicating better performance.


\begin{table}[h!]
\centering
\begin{tabular}{|l|c|p{8cm}|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
\( R^2 \)(R-Squared) & 0.8  \\
\hline
Mean Square Error (MSE) & 21 \\
\hline
\end{tabular}
\caption{Model performance metrics.}
\label{tab:model-performance}
\end{table}

\subsection*{Analysis}

The \(R^2\) value of 0.8 demonstrates that the model explains a substantial portion of the variance in the target variable, effectively capturing the relationships in the data.
The MSE value of 21 is acceptable because it aligns well with the scale of the target variable.


\section{Discussion}
The discussion section focuses on the main challenges/issues you had to overcome during the project. Outline what your approach does better than the ones you mentioned in your related work, and explain why. Do the same with issues where other solutions  outperform your own. Are there limitations to your approach? If so, what would you recommend towards removing/mitigating them? Given the experience you've gathered working on this project, are there other approaches that you feel are worth exploring?
\subsection{Limitations of the Approach}
One major limitation of the current implementation is its reliance on Rust without the Standard Library (stdlib). While this choice allows for reduced build and runtime, it significantly limits the ability to implement more complex machine learning models. This low-level implementation, while efficient for simpler models like Ridge Regression, imposes constraints on usability and feature expansion. For instance, without stdlib, basic functionalities like I/O and access to additional data structures are unavailable, reducing the flexibility of the implementation.

\subsection{Recommendations for Mitigation}
To overcome these limitations, it is recommended to:
\begin{itemize}
    \item Explore ways to selectively incorporate necessary parts of stdlib without significantly impacting performance.
    \item Investigate low-level optimization techniques to support complex models while maintaining efficiency.
    \item Utilize external libraries or frameworks designed for Rust to enhance capabilities without reintroducing excessive overhead.
\end{itemize}

\subsection{Next Steps}
Based on the current implementation and findings, the following steps are proposed for future work:
\begin{itemize}
    \item Perform benchmarking without precompiles to evaluate raw performance.
    \item Compare CPU vs. GPU performance to understand hardware utilization.
    \item Conduct memory usage benchmarking to optimize resource management.
    \item Carry out a qualitative analysis of the trade-offs between low-level implementation and stdlib inclusion.
\end{itemize}

These steps will provide deeper insights into the strengths and limitations of the approach and inform potential improvements in future iterations.


\section{Personal Contributions}

\subsection{Siddhartha Tuladhar}

\subsection{Lawrence Lim}

\subsection{Brandon Gao}

\section{Conclusion}

Give a clear, short, and informative summary of all your important results. Answer the initial question(s) or respond to what you wanted to do, as stated in your introduction. It can be a short table or a list, and possibly one or two short comments or explanations. 

Target a reader who may not have time to read the whole report yet, but needs the results or the conclusions immediately. This is a typical situation in real life. Some readers will read your introduction and skip to your conclusion first, and read the whole report only later (if at all).

You may also draw perspectives. What's missing? In what directions could your work be extended?

\newpage
\singlespacing
\bibliographystyle{IEEEtran}
\bibliography{references}


%------ To create Appendix with additional stuff -------%
%\newpage
%\appendix
%\section{Appendix}
%Put data files, CAD drawings, additional sketches, etc.

\end{document}
 
